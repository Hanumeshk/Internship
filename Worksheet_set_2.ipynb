{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85111fa",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "* You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44a9e1",
   "metadata": {},
   "source": [
    "# *This task will be done in following steps:*\n",
    "* 1. First get the webpage https://www.naukri.com/\n",
    "* 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "* 3. Then click the search button.\n",
    "* 4. Then scrape the data for the first 10 jobs results you get.\n",
    "* 5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af1f01",
   "metadata": {},
   "source": [
    "# Process begins for Installing required libraries and WebScraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "737fca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>NXP Semiconductors</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EY GDS Data Analyst-Finland based project</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>EY</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business &amp; Data Analyst - Alteryx (London)</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>Imaginative Brains LLP</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - Data and Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Intel</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - Data Science, 3 To 5 Years</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Rise Finconnect Private Limited</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>METRO Cash &amp; Carry</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genpact Hiring For Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data analyst/ data analytics / Business analys...</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Delhi /...</td>\n",
       "      <td>Leading US MNC into Analytics</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Titles  \\\n",
       "0                              Business Data Analyst   \n",
       "1          EY GDS Data Analyst-Finland based project   \n",
       "2         Business & Data Analyst - Alteryx (London)   \n",
       "3                  Data Analyst - Data and Analytics   \n",
       "4                                Senior Data Analyst   \n",
       "5                                Senior Data Analyst   \n",
       "6          Data Analyst - Data Science, 3 To 5 Years   \n",
       "7                    Data Analyst / Business Analyst   \n",
       "8                    Genpact Hiring For Data Analyst   \n",
       "9  data analyst/ data analytics / Business analys...   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Gurgaon/Gurugram, Bangalore/Bengaluru, Delhi /...   \n",
       "\n",
       "                      Company Name Experience  \n",
       "0               NXP Semiconductors    2-5 Yrs  \n",
       "1                               EY    0-1 Yrs  \n",
       "2           Imaginative Brains LLP   5-10 Yrs  \n",
       "3                            Intel    3-6 Yrs  \n",
       "4                         Flipkart    3-7 Yrs  \n",
       "5                          Walmart    4-7 Yrs  \n",
       "6  Rise Finconnect Private Limited    2-6 Yrs  \n",
       "7               METRO Cash & Carry    3-8 Yrs  \n",
       "8                          Genpact    2-6 Yrs  \n",
       "9    Leading US MNC into Analytics    2-7 Yrs  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#finding element for designation\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input \")  #job search bar\n",
    "search_field_designation.send_keys(\"Data Analyst\")   #enter degnation key to search\n",
    "#finding element for location field\n",
    "search_field_location=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[5]/div/div/div/input')\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "job_titles=[]\n",
    "comp_name=[]\n",
    "job_location=[]\n",
    "experience_req=[]\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")  #Scrap elements for job titles\n",
    "job_loc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\") #Scrap elements for job location\n",
    "job_company=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\") #Scrap elements for company name\n",
    "Job_exp=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\") #Scarp elements for experience\n",
    "\n",
    "\n",
    "for i in titles_tags:job_titles.append(i.text)  #for titles\n",
    "\n",
    "for i in job_company:comp_name.append(i.text)  #for company name\n",
    "  \n",
    "for i in job_loc:job_location.append(i.text)  #for job location  \n",
    "   \n",
    "for i in Job_exp:experience_req.append(i.text)#for experience   \n",
    "    \n",
    "#Making DataFrame\n",
    "df=pd.DataFrame({'Titles':job_titles,'Location':job_location,'Company Name':comp_name,'Experience':experience_req})\n",
    "Data_Analyst=df[0:10] #For top 10 job list\n",
    "Data_Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef96e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d16912",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location.\n",
    "You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e73b8f",
   "metadata": {},
   "source": [
    "# *This task will be done in following steps:*\n",
    "* 1. First get the webpage https://www.naukri.com/\n",
    "* 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "* 3. Then click the search button.\n",
    "* 4. Then scrape the data for the first 10 jobs results you get.\n",
    "* 5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6403f47f",
   "metadata": {},
   "source": [
    "# Process begins for WebScraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5bc707f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI Technologist Vacancy</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Opening with Wipro For Data Scientist posi...</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Chennai, Bang...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Immediate Joiners</td>\n",
       "      <td>Noida, Mumbai, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>Bristlecone</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist- AI/ML- R&amp;D</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Pune, Gurgaon/G...</td>\n",
       "      <td>EXL</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Urgent Hiring For AI Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Titles  \\\n",
       "0                            AI Technologist Vacancy   \n",
       "1  Job Opening with Wipro For Data Scientist posi...   \n",
       "2                                     Data Scientist   \n",
       "3                              Senior Data Scientist   \n",
       "4                              Senior Data Scientist   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                 Data Scientist - Immediate Joiners   \n",
       "8                         Data Scientist- AI/ML- R&D   \n",
       "9                Urgent Hiring For AI Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...   \n",
       "1  Kolkata, Hyderabad/Secunderabad, Chennai, Bang...   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...   \n",
       "7           Noida, Mumbai, Pune, Bangalore/Bengaluru   \n",
       "8  Noida, Hyderabad/Secunderabad, Pune, Gurgaon/G...   \n",
       "9  New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...   \n",
       "\n",
       "                         Company Name Experience  \n",
       "0                               Wipro   6-11 Yrs  \n",
       "1                               Wipro    2-7 Yrs  \n",
       "2                             Genpact    5-7 Yrs  \n",
       "3                             Walmart    5-9 Yrs  \n",
       "4                             Walmart    5-9 Yrs  \n",
       "5                             Genpact   7-12 Yrs  \n",
       "6                             Genpact    5-8 Yrs  \n",
       "7                         Bristlecone    5-8 Yrs  \n",
       "8                                 EXL    2-6 Yrs  \n",
       "9  Ashkom Media India Private Limited    1-4 Yrs  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#finding element for designation\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input \")  #job search bar\n",
    "search_field_designation.send_keys(\"Data Scientist\")   #enter degnation key to search\n",
    "#finding element for location field\n",
    "search_field_location=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[5]/div/div/div/input')\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Scraping required elements\n",
    "job_titles=[]\n",
    "comp_name=[]\n",
    "job_location=[]\n",
    "experience_req=[]\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")  #Scrap elements for job titles\n",
    "job_loc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\") #Scrap elements for job location\n",
    "job_company=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\") #Scrap elements for company name\n",
    "Job_exp=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\") #Scarp elements for experience\n",
    "\n",
    "\n",
    "for i in titles_tags:job_titles.append(i.text)  #for titles\n",
    "\n",
    "for i in job_company:comp_name.append(i.text)  #for company name\n",
    "  \n",
    "for i in job_loc:job_location.append(i.text)  #for job location  \n",
    "   \n",
    "for i in Job_exp:experience_req.append(i.text)#for experience   \n",
    "\n",
    "#Making DataFrame\n",
    "df=pd.DataFrame({'Titles':job_titles,'Location':job_location,'Company Name':comp_name,'Experience':experience_req})\n",
    "Data_Scientist=df[0:10] #For top 10 job list\n",
    "Data_Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f1ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d85ebeb",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below\n",
    "* You have to use the location and salary filter.\n",
    "* You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "* You have to scrape the job-title, job-location, company name, experience required.\n",
    "* The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effae28",
   "metadata": {},
   "source": [
    "# The task will be done as shown in the below steps:\n",
    "* 1. first get the webpage https://www.naukri.com/\n",
    "* 2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "* 3. Then click the search button.\n",
    "* 4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "* 5. Then scrape the data for the first 10 jobs results you get.\n",
    "* 6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94d537",
   "metadata": {},
   "source": [
    "# Process begins for Installing Selenium libraries and webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f59d75dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Opening with Wipro For Data Scientist posi...</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Chennai, Bang...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Machine learning AI</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>Teq Analytics</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist -Machine Learning with Python</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>SS Supply Chain Solutions Pvt. Ltd. (3SC)</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Predictive Analytics</td>\n",
       "      <td>Noida, Mumbai, Chandigarh, Hyderabad/Secundera...</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Internet Jobs - II</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>Jobs Territory</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Machine Learning Engineer | Data Engineer | Da...</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru, Delhi / NCR(Sect...</td>\n",
       "      <td>Tidyquant (OPC) Private Limited</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dot Net Developer</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Nibha Infotech Private Limited</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Titles  \\\n",
       "0  Job Opening with Wipro For Data Scientist posi...   \n",
       "1               Data Scientist - Machine learning AI   \n",
       "2       Data Scientist -Machine Learning with Python   \n",
       "3                                     Data Scientist   \n",
       "4                     Data Scientist - MIND Infotech   \n",
       "5                     Data Scientist - MIND Infotech   \n",
       "6              Data Scientist - Predictive Analytics   \n",
       "7                Data Scientist - Internet Jobs - II   \n",
       "8  Machine Learning Engineer | Data Engineer | Da...   \n",
       "9                                  Dot Net Developer   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Kolkata, Hyderabad/Secunderabad, Chennai, Bang...   \n",
       "1  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...   \n",
       "2    Noida, New Delhi, Gurgaon/Gurugram, Delhi / NCR   \n",
       "3        Pune, Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "4                                              Noida   \n",
       "5                                              Noida   \n",
       "6  Noida, Mumbai, Chandigarh, Hyderabad/Secundera...   \n",
       "7  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...   \n",
       "8  Chennai, Bangalore/Bengaluru, Delhi / NCR(Sect...   \n",
       "9                      Gurgaon/Gurugram, Delhi / NCR   \n",
       "\n",
       "                                Company Name Experience  \n",
       "0                                      Wipro    2-7 Yrs  \n",
       "1                              Teq Analytics    3-8 Yrs  \n",
       "2                                    Genpact    1-4 Yrs  \n",
       "3  SS Supply Chain Solutions Pvt. Ltd. (3SC)    2-5 Yrs  \n",
       "4   MOTHERSONSUMI INFOTECH & DESIGNS LIMITED    4-8 Yrs  \n",
       "5   MOTHERSONSUMI INFOTECH & DESIGNS LIMITED    4-8 Yrs  \n",
       "6                               Confidential    1-6 Yrs  \n",
       "7                             Jobs Territory    3-6 Yrs  \n",
       "8            Tidyquant (OPC) Private Limited    1-3 Yrs  \n",
       "9             Nibha Infotech Private Limited    3-8 Yrs  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#finding element for designation\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input \")  #job search bar\n",
    "search_field_designation.send_keys(\"Data Scientist\")   #enter degnation key to search\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "#Apply location filter Delhi/NCR\n",
    "apply_filter_location=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/p')\n",
    "apply_filter_location.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Apply salary filter 3-6Lackhs\n",
    "apply_salary_filter=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/p')\n",
    "apply_salary_filter.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping required elements\n",
    "job_titles=[]\n",
    "comp_name=[]\n",
    "job_location=[]\n",
    "experience_req=[]\n",
    "\n",
    "p=driver.find_elements_by_xpath\n",
    "titles_tags=p(\"//a[@class='title fw500 ellipsis']\")  #Scrap elements for job titles\n",
    "job_loc=p(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\") #Scrap elements for job location\n",
    "job_company=p(\"//a[@class='subTitle ellipsis fleft']\") #Scrap elements for company name\n",
    "Job_exp=p(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\") #Scarp elements for experience\n",
    "\n",
    "\n",
    "\n",
    "for i in titles_tags:\n",
    "    job_titles.append(i.text)  #for titles\n",
    "\n",
    "for i in job_company:\n",
    "    comp_name.append(i.text)  #for company name\n",
    "  \n",
    "for i in job_loc:\n",
    "    job_location.append(i.text)  #for job location  \n",
    "   \n",
    "for i in Job_exp:\n",
    "    experience_req.append(i.text)#for experience   \n",
    "\n",
    "#Making DataFrame\n",
    "Filter=pd.DataFrame({'Titles':job_titles,'Location':job_location,'Company Name':comp_name,'Experience':experience_req})\n",
    "Filters_loc_sal=Filter[0:10] #For top 10 job list\n",
    "Filters_loc_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91bcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d701ce7",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "* 1. Brand\n",
    "* 2. Product Description\n",
    "* 3. Price\n",
    "* The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceccb07",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "ASSIGNMENT 2\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87f30925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price offer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mi</td>\n",
       "      <td>Polarized Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹649</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹199</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹199</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹699</td>\n",
       "      <td>30% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹219</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>₹283</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>Mirrored, UV Protection, Riding Glasses, Other...</td>\n",
       "      <td>₹299</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lee Topper</td>\n",
       "      <td>Riding Glasses Wrap-around Sunglasses (Free Size)</td>\n",
       "      <td>₹299</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹195</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Company                                        Description  \\\n",
       "0                  Mi           Polarized Aviator Sunglasses (Free Size)   \n",
       "1                SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "2              PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "3            Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4            Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "..                ...                                                ...   \n",
       "95             PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "96             SUNBEE  UV Protection, Polarized Wayfarer Sunglasses (...   \n",
       "97          New Specs  Mirrored, UV Protection, Riding Glasses, Other...   \n",
       "98         Lee Topper  Riding Glasses Wrap-around Sunglasses (Free Size)   \n",
       "99  SHAAH COLLECTIONS  UV Protection, Polarized, Mirrored Rectangular...   \n",
       "\n",
       "   Price Price offer  \n",
       "0   ₹649     45% off  \n",
       "1   ₹199     84% off  \n",
       "2   ₹199     87% off  \n",
       "3   ₹639     20% off  \n",
       "4   ₹699     30% off  \n",
       "..   ...         ...  \n",
       "95  ₹219     86% off  \n",
       "96  ₹283     78% off  \n",
       "97  ₹299     81% off  \n",
       "98  ₹299     88% off  \n",
       "99  ₹195     88% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.flipkart.com/')\n",
    "p=driver.find_elements_by_xpath\n",
    "\n",
    "#finding element for designation\n",
    "search_field_designation=driver.find_element_by_class_name(\"_3704LK\")  #job search bar\n",
    "pop_up_close=driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "pop_up_close.click()\n",
    "driver.maximize_window()\n",
    "search_field_designation.send_keys(\"Sunglasses\")   #enter degnation key to search\n",
    "search=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search.submit()\n",
    "time.sleep(3)\n",
    "\n",
    "companies=[]\n",
    "desc=[]\n",
    "prod_price=[]\n",
    "prod_offer=[]\n",
    "\n",
    "for i in range(3):\n",
    "    #print(i+1)\n",
    "    company=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\") #For scraping company names\n",
    "    prod_des=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")  #For scraping product disription\n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")   #For Scraping prices\n",
    "    offer=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")   # For Scraping price offers\n",
    "\n",
    "    for p in company:\n",
    "        companies.append(p.text)\n",
    "    for p in prod_des:\n",
    "        desc.append(p.text)\n",
    "    for p in price:\n",
    "        prod_price.append(p.text)\n",
    "    for p in offer:\n",
    "        prod_offer.append(p.text)\n",
    "        time.sleep(3)\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\") # For moving next page\n",
    "next_button.click()\n",
    "    \n",
    "    \n",
    "sunglass=pd.DataFrame({'Company':companies,'Description':desc,'Price':prod_price,'Price offer':prod_offer})\n",
    "Sunglasses=sunglass.iloc[0:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc590d",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02579b",
   "metadata": {},
   "source": [
    "# As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "* 1. Rating\n",
    "* 2. Review summary\n",
    "* 3. Full review\n",
    "* 4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5126e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='col col-7-12']\"}\n  (Session info: chrome=102.0.5005.115)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00CFD953+2414931]\n\tOrdinal0 [0x00C8F5E1+1963489]\n\tOrdinal0 [0x00B7C6B8+837304]\n\tOrdinal0 [0x00BA9500+1021184]\n\tOrdinal0 [0x00BA979B+1021851]\n\tOrdinal0 [0x00BD6502+1205506]\n\tOrdinal0 [0x00BC44E4+1131748]\n\tOrdinal0 [0x00BD4812+1198098]\n\tOrdinal0 [0x00BC42B6+1131190]\n\tOrdinal0 [0x00B9E860+976992]\n\tOrdinal0 [0x00B9F756+980822]\n\tGetHandleVerifier [0x00F6CC62+2510274]\n\tGetHandleVerifier [0x00F5F760+2455744]\n\tGetHandleVerifier [0x00D8EABA+551962]\n\tGetHandleVerifier [0x00D8D916+547446]\n\tOrdinal0 [0x00C95F3B+1990459]\n\tOrdinal0 [0x00C9A898+2009240]\n\tOrdinal0 [0x00C9A985+2009477]\n\tOrdinal0 [0x00CA3AD1+2046673]\n\tBaseThreadInitThunk [0x766DFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x776C7A7E+286]\n\tRtlGetAppContainerNamedObjectPath [0x776C7A4E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13420/3591133970.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mmobile_det\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//div[@class='col col-7-12']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mmobile_det\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[1;34m(self, xpath)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         )\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWebElement\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[0;32m   1252\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             'value': value})['value']\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    432\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='col col-7-12']\"}\n  (Session info: chrome=102.0.5005.115)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00CFD953+2414931]\n\tOrdinal0 [0x00C8F5E1+1963489]\n\tOrdinal0 [0x00B7C6B8+837304]\n\tOrdinal0 [0x00BA9500+1021184]\n\tOrdinal0 [0x00BA979B+1021851]\n\tOrdinal0 [0x00BD6502+1205506]\n\tOrdinal0 [0x00BC44E4+1131748]\n\tOrdinal0 [0x00BD4812+1198098]\n\tOrdinal0 [0x00BC42B6+1131190]\n\tOrdinal0 [0x00B9E860+976992]\n\tOrdinal0 [0x00B9F756+980822]\n\tGetHandleVerifier [0x00F6CC62+2510274]\n\tGetHandleVerifier [0x00F5F760+2455744]\n\tGetHandleVerifier [0x00D8EABA+551962]\n\tGetHandleVerifier [0x00D8D916+547446]\n\tOrdinal0 [0x00C95F3B+1990459]\n\tOrdinal0 [0x00C9A898+2009240]\n\tOrdinal0 [0x00C9A985+2009477]\n\tOrdinal0 [0x00CA3AD1+2046673]\n\tBaseThreadInitThunk [0x766DFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x776C7A7E+286]\n\tRtlGetAppContainerNamedObjectPath [0x776C7A4E+238]\n"
     ]
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "\n",
    "#finding element for designation\n",
    "search_field_designation=driver.find_element_by_class_name(\"_3704LK\")  #job search bar\n",
    "pop_up_close=driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "pop_up_close.click()\n",
    "search_field_designation.send_keys(\"iphone 11\")   #enter degnation key to search\n",
    "search=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search.submit()\n",
    "time.sleep(2)\n",
    "mobile_det=driver.find_element_by_xpath(\"//div[@class='col col-7-12']\")\n",
    "mobile_det.click()\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath(\"//div[@class='_2tsNFb']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb4367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "page_urls=[]\n",
    "rating=[]\n",
    "no_rating=[]\n",
    "for page in range(0,3):\n",
    "    url1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for t in url1:\n",
    "        page_urls.append(t.get_attribute('href'))\n",
    "    page_urls \n",
    "    \n",
    "    time.sleep(3)\n",
    "  #  for scraping no_rating\n",
    "no_ratings=driver.find_elements_by_xpath('//a[@id=\"acrCustomerReviewLink\"]')\n",
    "for l in no_ratings:\n",
    "    if l.text is None:\n",
    "        no_rating.append(\"--\")\n",
    "    else:\n",
    "            no_rating.append(l.text)\n",
    "     \n",
    "    time.sleep(3)\n",
    "  # for scraping rating\n",
    "ratings=driver.find_elements_by_xpath('//span[@data-hook=\"acr-average-stars-rating-text\"]')\n",
    "for m in ratings:\n",
    "    rating.append(m.text)\n",
    "    time.sleep(3)\n",
    "print(len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa3eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2de7a82",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "* 1. Brand\n",
    "* 2. Product Description\n",
    "* 3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ae2d2f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIRDE</td>\n",
       "      <td>₹299</td>\n",
       "      <td>Stylish Comfortable Lightweight, Breathable Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>₹399</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹199</td>\n",
       "      <td>Lightweight Pack Of 1 Trendy Sneakers Sneakers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>₹374</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>₹399</td>\n",
       "      <td>Casual Sneakers Green Shoes For Men And Boys S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>D-SNEAKERZ</td>\n",
       "      <td>₹284</td>\n",
       "      <td>casual for men Sneakers For Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CAMPUS</td>\n",
       "      <td>₹709</td>\n",
       "      <td>Puma Smash v2 L Sneakers For Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Dicy</td>\n",
       "      <td>₹284</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>₹474</td>\n",
       "      <td>Casual Sneakers White Shoes For Men Sneakers F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>U.S. POLO ASSN.</td>\n",
       "      <td>₹1,295</td>\n",
       "      <td>Puma Smash Vulc Sneakers For Men</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Company   Price                                        Description\n",
       "0             BIRDE    ₹299  Stylish Comfortable Lightweight, Breathable Wa...\n",
       "1            Layasa    ₹399                                   Sneakers For Men\n",
       "2            BRUTON    ₹199  Lightweight Pack Of 1 Trendy Sneakers Sneakers...\n",
       "3          Magnolia    ₹374                                   Sneakers For Men\n",
       "4      Robbie jones    ₹399  Casual Sneakers Green Shoes For Men And Boys S...\n",
       "..              ...     ...                                                ...\n",
       "95       D-SNEAKERZ    ₹284                    casual for men Sneakers For Men\n",
       "96           CAMPUS    ₹709                   Puma Smash v2 L Sneakers For Men\n",
       "97             Dicy    ₹284                                   Sneakers For Men\n",
       "98         ASTEROID    ₹474  Casual Sneakers White Shoes For Men Sneakers F...\n",
       "99  U.S. POLO ASSN.  ₹1,295                   Puma Smash Vulc Sneakers For Men\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "pop_up_close=driver.find_element_by_xpath('/html/body/div[2]/div/div/button') #To close ligin popup\n",
    "pop_up_close.click()\n",
    "time.sleep(2)\n",
    "search_field_designation=driver.find_element_by_class_name(\"_3704LK\")  #job search bar\n",
    "search_field_designation.send_keys(\"sneakers\")   #enter items to search\n",
    "time.sleep(3)\n",
    "search=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input') #For dearch button\n",
    "search.submit()\n",
    "time.sleep(3)\n",
    "\n",
    "brands=[]\n",
    "desc=[]\n",
    "prices=[]\n",
    "\n",
    "#Scraping deatails\n",
    "for i in range(3):\n",
    "    brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")  #For scraping brand names\n",
    "    prod_dec=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")   #For scraping products discription\n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")   #For Scraping prices\n",
    "    \n",
    "    for p in brand:\n",
    "        brands.append(p.text)\n",
    "    for p in prod_dec:\n",
    "        desc.append(p.text)\n",
    "    for p in price:\n",
    "        prices.append(p.text)\n",
    "        time.sleep(3)\n",
    "Next=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\") #For moving next page\n",
    "Next.click()\n",
    "\n",
    "sneaker=pd.DataFrame({'Company':brands,'Price':prices})\n",
    "first_sneakers=sneaker.iloc[0:100]\n",
    "prod_desc=pd.DataFrame({'Description':desc})\n",
    "first_des=prod_desc.iloc[0:100]\n",
    "sneakers=pd.concat([first_sneakers,first_des],axis=1)\n",
    "#print(len(brands),len(prices),len(desc))\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43808bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab071f8",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "* Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. \n",
    "* The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5f6b8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Go Walk 5 Walking Shoes</td>\n",
       "      <td>Rs. 8499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Men UA Charged Vantage 2 Run</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Ferrari R-Cat Machina Sneakers</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Eternity Nitro Running Shoes</td>\n",
       "      <td>Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Electrify Nitro Running Shoes</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Perforations Sneakers</td>\n",
       "      <td>Rs. 13990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 10990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Women Hovr Sonic SE Run Shoes</td>\n",
       "      <td>Rs. 8999Rs. 9999(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 12990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Women KarlieKlossX9000 Running</td>\n",
       "      <td>Rs. 13999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Names                     Description                      Price\n",
       "0          Skechers     Men Go Walk 5 Walking Shoes                   Rs. 8499\n",
       "1      UNDER ARMOUR    Men UA Charged Vantage 2 Run                   Rs. 7999\n",
       "2   PUMA Motorsport  Ferrari R-Cat Machina Sneakers                   Rs. 7499\n",
       "3              Puma    Eternity Nitro Running Shoes                  Rs. 12999\n",
       "4              Puma   Electrify Nitro Running Shoes                   Rs. 9999\n",
       "..              ...                             ...                        ...\n",
       "95             Geox       Men Perforations Sneakers                  Rs. 13990\n",
       "96             Geox            Men Leather Sneakers                  Rs. 10990\n",
       "97     UNDER ARMOUR   Women Hovr Sonic SE Run Shoes  Rs. 8999Rs. 9999(10% OFF)\n",
       "98             Geox            Men Leather Sneakers                  Rs. 12990\n",
       "99           ADIDAS  Women KarlieKlossX9000 Running                  Rs. 13999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up myntra website on automated chrome window\n",
    "driver.get('https://www.myntra.com/shoes')  #getting myntra driver link\n",
    "driver.maximize_window()\n",
    "#Apply filter color as black\n",
    "filter2=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]\") \n",
    "filter2.click()\n",
    "time.sleep(3)\n",
    "#Apply filter salary from Rupees 7179 to 14119\n",
    "filter1=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]\")\n",
    "filter1.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Scraping details\n",
    "names=[]\n",
    "desc=[]\n",
    "prices=[]\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    brand_name=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\") #For scraping brand names\n",
    "    brand_des=driver.find_elements_by_xpath(\"//h4[@class='product-product']\") #For Scraping product description\n",
    "    brand_price=driver.find_elements_by_xpath(\"//div[@class='product-price']\") # For scraping prices\n",
    "    \n",
    "    for b in brand_name:\n",
    "        names.append(b.text)\n",
    "    for b in brand_des:\n",
    "        desc.append(b.text)\n",
    "    for b in brand_price:\n",
    "        prices.append(b.text)\n",
    "#For moving next page        \n",
    "page2=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[12]\")\n",
    "page2.click()\n",
    "\n",
    "shoes=pd.DataFrame({'Names':names,'Description':desc,'Price':prices})\n",
    "shoes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c330f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73773b38",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3792fd2",
   "metadata": {},
   "source": [
    "# After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "* 1. Title\n",
    "* 2. Ratings\n",
    "* 3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac1e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo Yoga 9 11th Gen Intel Core i7 14\" 4K Ul...</td>\n",
       "      <td>1,72,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...</td>\n",
       "      <td>72,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo IdeaPad 5 Pro 11th Gen Intel Core i7 14...</td>\n",
       "      <td>75,309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>57,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LG Gram 17 Intel Evo 11th Gen i7 Thin &amp; Light ...</td>\n",
       "      <td>88,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021) 15.6-inch (39.62 cm...</td>\n",
       "      <td>92,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title     Price\n",
       "0  Lenovo Yoga 9 11th Gen Intel Core i7 14\" 4K Ul...  1,72,990\n",
       "1  MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...    72,490\n",
       "2  Lenovo IdeaPad 5 Pro 11th Gen Intel Core i7 14...    75,309\n",
       "3  ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...    57,990\n",
       "4  LG Gram 17 Intel Evo 11th Gen i7 Thin & Light ...    88,490\n",
       "5  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...    86,990\n",
       "6  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....    86,990\n",
       "7  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...    89,990\n",
       "8  HP Pavilion x360 11th Gen Intel Core i7 14 inc...    84,990\n",
       "9  ASUS TUF Gaming F15 (2021) 15.6-inch (39.62 cm...    92,990"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up amazon.com website on automated chrome window\n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.maximize_window()   #To maximize window\n",
    "#searching element by items in search field\n",
    "driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute']\").send_keys(\"Laptop\")  \n",
    "driver.find_element_by_xpath(\"//input[@value='Go']\").click()\n",
    "time.sleep(3)\n",
    "#Applying filter CPU type as corei7\n",
    "core=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[13]/span/a\")\n",
    "core.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping elements\n",
    "\n",
    "Titles=[]\n",
    "Ratings=[]\n",
    "Prices=[]\n",
    "\n",
    "\n",
    "laptop=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in laptop:\n",
    "    Titles.append(i.text)\n",
    "pric=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for i in pric:\n",
    "    Prices.append(i.text)\n",
    "lap_rat=driver.find_elements_by_xpath(\"//span[@class='a-size-base a-nowrap']\")\n",
    "for i in lap_rat:\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "Laptops=pd.DataFrame({'Title':Titles,'Price':Prices})\n",
    "#print(len(Titles),len(Prices),len(Ratings))\n",
    "Laptops[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435c88e",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location.\n",
    "\n",
    "* You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccc067",
   "metadata": {},
   "source": [
    "# This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37beaef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hcl Technologies Limited</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXL Services.com ( I ) Pvt. Ltd.</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Paytm</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Om Software Internet Solutions Private Limited</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Paytm</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Company Rating\n",
       "0                   GENPACT India Private Limited    4.0\n",
       "1  Optum Global Solutions (India) Private Limited    4.2\n",
       "2                   GENPACT India Private Limited    4.0\n",
       "3                        Hcl Technologies Limited    3.9\n",
       "4                EXL Services.com ( I ) Pvt. Ltd.    3.9\n",
       "5                                           Paytm    3.7\n",
       "6  Om Software Internet Solutions Private Limited    4.5\n",
       "7                                           Paytm    3.7\n",
       "8        MOTHERSONSUMI INFOTECH & DESIGNS LIMITED    3.3\n",
       "9              Ashkom Media India Private Limited    3.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up ambition.com website on automated chrome window\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "driver.maximize_window()\n",
    "driver.find_element_by_xpath(\"//a[@class='link jobs']\").click() #For clicking on jobs to search\n",
    "driver.find_element_by_xpath(\"//input[@class='input tt-input']\").send_keys(\"Data Scientist\") #enter a key as data scientist to search\n",
    "driver.find_element_by_xpath(\"//button[@class='ab_btn search-btn round']\").click() #click search button\n",
    "time.sleep(3)\n",
    "#clicking search location\n",
    "filt=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]\") \n",
    "filt.click()\n",
    "#Applying filter as location Noida\n",
    "filt2=driver.find_element_by_xpath(\"//input[@id='location_Noida']\")\n",
    "filt2.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Scraping details\n",
    "\n",
    "Names=[]\n",
    "Days=[]\n",
    "Rating=[]\n",
    "\n",
    "comp=driver.find_elements_by_xpath(\"//p[@class='company body-medium']\")  #For scraping company names\n",
    "for i in comp:\n",
    "    Names.append(i.text)\n",
    "day=driver.find_elements_by_xpath(\"//span[@class='body-small-l']\")    #For scraping no.of days ago posted\n",
    "for i in day:\n",
    "    Days.append(i.text)\n",
    "rate=driver.find_elements_by_xpath(\"//span[@class='body-small']\")   #For scraping rating of job\n",
    "for i in rate:\n",
    "    Rating.append(i.text)\n",
    "#print(len(Names),len(Days),len(Rating))\n",
    "#Day=pd.DataFrame({'Posted Days':Days})\n",
    "amb=pd.DataFrame({'Company':Names,'Rating':Rating})\n",
    "amb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61c84f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13d ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>via naukri.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               day\n",
       "0           6d ago\n",
       "1   via naukri.com\n",
       "2          14d ago\n",
       "3   via naukri.com\n",
       "4          13d ago\n",
       "5   via naukri.com\n",
       "6          16d ago\n",
       "7   via naukri.com\n",
       "8          27d ago\n",
       "9   via naukri.com\n",
       "10          9d ago\n",
       "11  via naukri.com\n",
       "12         17d ago\n",
       "13  via naukri.com\n",
       "14         27d ago\n",
       "15  via naukri.com\n",
       "16         14d ago\n",
       "17  via naukri.com\n",
       "18         13d ago\n",
       "19  via naukri.com"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Post=pd.DataFrame(Days,columns=['Post Days'])\n",
    "post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b44c6",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4576c3",
   "metadata": {},
   "source": [
    "# The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "ASSIGNMENT 2\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aad5601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "! pip install selenium\n",
    "#importing all the neccessary libraries\n",
    "import selenium          #library that is used to work with selenium\n",
    "import numpy as np       #for dealing with arrays\n",
    "import pandas as pd      #to create dataframe\n",
    "from selenium import webdriver #importing webdriver module from selenium to openup automated chrome window\n",
    "import time                    #use to stop search engine for few seconds\n",
    "import warnings             #to ignore any sort of warnings\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore the unnecessary warnings\n",
    "\n",
    "#Lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\DELL\\Desktop\\Data Science\\CSV Files\\chromedriver.exe\")\n",
    "\n",
    "#opening up ambition.com website on automated chrome window\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "driver.maximize_window()\n",
    "driver.find_element_by_xpath(\"//a[@class='link salaries']\").click()  #searching for salary based jobs\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath(\"//input[@class='tt-input']\").send_keys(\"Data Scientist\")  #Searching for data scientist jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d8eea025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Total salary record</th>\n",
       "      <th>2</th>\n",
       "      <th>Average salary</th>\n",
       "      <th>Minimum salary</th>\n",
       "      <th>Maximum salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 12 salaries</td>\n",
       "      <td>3 yrs exp</td>\n",
       "      <td>₹ 30.2L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 36.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 33 salaries</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "      <td>₹ 20.6L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 25.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American Express</td>\n",
       "      <td>based on 10 salaries</td>\n",
       "      <td>4 yrs exp</td>\n",
       "      <td>₹ 19.9L</td>\n",
       "      <td>₹ 14.1L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS</td>\n",
       "      <td>based on 15 salaries</td>\n",
       "      <td>2 yrs exp</td>\n",
       "      <td>₹ 16.7L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optum</td>\n",
       "      <td>based on 33 salaries</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "      <td>₹ 16.1L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.6L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reliance Jio</td>\n",
       "      <td>based on 21 salaries</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "      <td>₹ 15.7L</td>\n",
       "      <td>₹ 5.6L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 83 salaries</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "      <td>₹ 15.4L</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>based on 50 salaries</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "      <td>₹ 14.8L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>based on 57 salaries</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 8.3L</td>\n",
       "      <td>₹ 21.1L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>based on 10 salaries</td>\n",
       "      <td>4 yrs exp</td>\n",
       "      <td>₹ 13.3L</td>\n",
       "      <td>₹ 8.9L</td>\n",
       "      <td>₹ 21.0L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Company   Total salary record            2 Average salary  \\\n",
       "0            Walmart  based on 12 salaries    3 yrs exp        ₹ 30.2L   \n",
       "1           Ab Inbev  based on 33 salaries  3-4 yrs exp        ₹ 20.6L   \n",
       "2   American Express  based on 10 salaries    4 yrs exp        ₹ 19.9L   \n",
       "3                 ZS  based on 15 salaries    2 yrs exp        ₹ 16.7L   \n",
       "4              Optum  based on 33 salaries  3-4 yrs exp        ₹ 16.1L   \n",
       "5       Reliance Jio  based on 21 salaries  3-4 yrs exp        ₹ 15.7L   \n",
       "6  Fractal Analytics  based on 83 salaries  2-4 yrs exp        ₹ 15.4L   \n",
       "7    Tiger Analytics  based on 50 salaries  2-4 yrs exp        ₹ 14.8L   \n",
       "8       UnitedHealth  based on 57 salaries  2-4 yrs exp        ₹ 14.0L   \n",
       "9        EXL Service  based on 10 salaries    4 yrs exp        ₹ 13.3L   \n",
       "\n",
       "  Minimum salary Maximum salary  \n",
       "0        ₹ 25.0L        ₹ 36.0L  \n",
       "1        ₹ 15.0L        ₹ 25.5L  \n",
       "2        ₹ 14.1L        ₹ 25.0L  \n",
       "3        ₹ 11.0L        ₹ 22.0L  \n",
       "4        ₹ 11.0L        ₹ 22.6L  \n",
       "5         ₹ 5.6L        ₹ 26.2L  \n",
       "6        ₹ 10.0L        ₹ 22.0L  \n",
       "7         ₹ 9.0L        ₹ 20.0L  \n",
       "8         ₹ 8.3L        ₹ 21.1L  \n",
       "9         ₹ 8.9L        ₹ 21.0L  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scraping Details\n",
    "\n",
    "comp_names=[]\n",
    "avg_sal=[]\n",
    "min_sal=[]\n",
    "exp_req=[]\n",
    "\n",
    "comp=driver.find_elements_by_xpath(\"//div[@class='name']\")  #For scraping company names and total salary records\n",
    "for i in comp:\n",
    "    comp_names.append(i.text)\n",
    "\n",
    "avg=driver.find_elements_by_xpath(\"//p[@class='averageCtc']\")  #For scraping average salary\n",
    "for i in avg:\n",
    "    avg_sal.append(i.text)\n",
    "Min=driver.find_elements_by_xpath(\"//div[@class='salary-values']\")  #For scraping minimum and maximum salaries\n",
    "for i in Min:\n",
    "    min_sal.append(i.text)\n",
    "exp=driver.find_elements_by_xpath(\"//div[@class='salaries sbold-list-header']\") #For scraping experience\n",
    "for i in exp:\n",
    "    exp_req.append(i.text)\n",
    "    \n",
    "print(len(comp_names),len(avg_sal),len(min_sal),len(exp_req))\n",
    "names=pd.DataFrame(comp_names,columns=['names'])\n",
    "sal=pd.DataFrame(min_sal,columns=['sal'])\n",
    "salary=sal['sal'].str.split('\\n',expand=True)\n",
    "salary.columns=['Minimum salary','Maximum salary']\n",
    "name=names['names'].str.split('\\n',expand=True)\n",
    "name.columns=['Company','Total salary record']\n",
    "ex=pd.DataFrame(exp_req,columns=['exp'])\n",
    "years=ex['exp'].str.split('\\n',expand=True)\n",
    "year=years.loc[: ,2]\n",
    "year.columns=[\"Experience\"]\n",
    "average=pd.DataFrame(avg_sal,columns=['Average salary'])\n",
    "Ambition=pd.concat([name,year,average,salary],axis=1)\n",
    "Ambition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3f407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
